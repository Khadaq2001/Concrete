{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qinxianhan/.conda/envs/singleCell/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import utils\n",
    "import time\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from concreteNet import ConcreteAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../data/filtered_gene_bc_matrices/hg19\"\n",
    "device = utils.get_device()\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2700, 4758)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = utils.anndata_load(file_path)\n",
    "adata = utils.anndata_preprocess(adata)\n",
    "adata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, loss, dataloader, num_epoch, learning_rate, weight_decay, device):\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
    "    start_time  = time.time()\n",
    "    net.train()\n",
    "    for epoch in range(num_epoch):\n",
    "        train_loss = 0\n",
    "        for data in dataloader:\n",
    "            input = data.to(device, non_blocking = True)\n",
    "            optimizer.zero_grad()\n",
    "            reconstruction, _ ,_ = net(input)\n",
    "            l = loss(reconstruction, input)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += l\n",
    "        process_time = time.time() - start_time\n",
    "        train_loss /= len(dataloader)\n",
    "        print(\"Epoch: %d ; Loss %.5f; Time: %.2f s\" %(epoch, train_loss, process_time))\n",
    "\n",
    "#def inference(net, dataloader):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(input_dim, k, hidden_dim,device, temperature):\n",
    "    return ConcreteAutoencoder(input_dim, k,hidden_dim,device, temperature)\n",
    "def load_adata(adata, batch_size):\n",
    "    return utils.data_loader(adata, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch, learning_rate, weight_dacay, batch_size, temperature = 128, 0.05, 0, 256, 0.1\n",
    "input_dim = adata.n_vars\n",
    "loss = nn.MSELoss()\n",
    "net = get_net(input_dim=input_dim, k=1000,hidden_dim=128,device=device, temperature=temperature)\n",
    "dataloader=load_adata(adata=adata, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 ; Loss 4.04366; Time: 0.65 s\n",
      "Epoch: 1 ; Loss 0.72354; Time: 1.30 s\n",
      "Epoch: 2 ; Loss 0.59253; Time: 1.88 s\n",
      "Epoch: 3 ; Loss 0.49475; Time: 2.48 s\n",
      "Epoch: 4 ; Loss 0.43796; Time: 3.21 s\n",
      "Epoch: 5 ; Loss 0.40687; Time: 4.19 s\n",
      "Epoch: 6 ; Loss 0.38934; Time: 4.85 s\n",
      "Epoch: 7 ; Loss 0.37917; Time: 5.50 s\n",
      "Epoch: 8 ; Loss 0.37315; Time: 6.09 s\n",
      "Epoch: 9 ; Loss 0.36934; Time: 6.62 s\n",
      "Epoch: 10 ; Loss 0.36718; Time: 7.13 s\n",
      "Epoch: 11 ; Loss 0.36606; Time: 7.70 s\n",
      "Epoch: 12 ; Loss 0.36552; Time: 8.21 s\n",
      "Epoch: 13 ; Loss 0.36512; Time: 8.81 s\n",
      "Epoch: 14 ; Loss 0.36519; Time: 9.29 s\n",
      "Epoch: 15 ; Loss 0.36473; Time: 9.87 s\n",
      "Epoch: 16 ; Loss 0.36477; Time: 10.36 s\n",
      "Epoch: 17 ; Loss 0.36461; Time: 10.85 s\n",
      "Epoch: 18 ; Loss 0.36446; Time: 11.32 s\n",
      "Epoch: 19 ; Loss 0.36456; Time: 11.97 s\n",
      "Epoch: 20 ; Loss 0.36443; Time: 12.53 s\n",
      "Epoch: 21 ; Loss 0.36475; Time: 13.20 s\n",
      "Epoch: 22 ; Loss 0.36464; Time: 13.78 s\n",
      "Epoch: 23 ; Loss 0.36498; Time: 14.39 s\n",
      "Epoch: 24 ; Loss 0.36455; Time: 15.02 s\n",
      "Epoch: 25 ; Loss 0.36503; Time: 15.59 s\n",
      "Epoch: 26 ; Loss 0.36460; Time: 16.37 s\n",
      "Epoch: 27 ; Loss 0.36467; Time: 17.15 s\n",
      "Epoch: 28 ; Loss 0.36464; Time: 17.92 s\n",
      "Epoch: 29 ; Loss 0.36458; Time: 18.51 s\n",
      "Epoch: 30 ; Loss 0.36487; Time: 19.01 s\n",
      "Epoch: 31 ; Loss 0.36468; Time: 19.56 s\n",
      "Epoch: 32 ; Loss 0.36460; Time: 20.28 s\n",
      "Epoch: 33 ; Loss 0.36472; Time: 20.83 s\n",
      "Epoch: 34 ; Loss 0.36456; Time: 21.42 s\n",
      "Epoch: 35 ; Loss 0.36456; Time: 22.00 s\n",
      "Epoch: 36 ; Loss 0.36468; Time: 22.58 s\n",
      "Epoch: 37 ; Loss 0.36497; Time: 23.15 s\n",
      "Epoch: 38 ; Loss 0.36425; Time: 23.64 s\n",
      "Epoch: 39 ; Loss 0.36471; Time: 24.18 s\n",
      "Epoch: 40 ; Loss 0.36514; Time: 24.78 s\n",
      "Epoch: 41 ; Loss 0.36456; Time: 25.29 s\n",
      "Epoch: 42 ; Loss 0.36490; Time: 25.82 s\n",
      "Epoch: 43 ; Loss 0.36458; Time: 26.35 s\n",
      "Epoch: 44 ; Loss 0.36497; Time: 26.94 s\n",
      "Epoch: 45 ; Loss 0.36472; Time: 27.45 s\n",
      "Epoch: 46 ; Loss 0.36414; Time: 28.15 s\n",
      "Epoch: 47 ; Loss 0.36454; Time: 28.84 s\n",
      "Epoch: 48 ; Loss 0.36477; Time: 29.68 s\n",
      "Epoch: 49 ; Loss 0.36490; Time: 30.51 s\n",
      "Epoch: 50 ; Loss 0.36482; Time: 31.28 s\n",
      "Epoch: 51 ; Loss 0.36464; Time: 31.80 s\n",
      "Epoch: 52 ; Loss 0.36496; Time: 32.37 s\n",
      "Epoch: 53 ; Loss 0.36477; Time: 32.91 s\n",
      "Epoch: 54 ; Loss 0.36470; Time: 33.54 s\n",
      "Epoch: 55 ; Loss 0.36478; Time: 34.05 s\n",
      "Epoch: 56 ; Loss 0.36473; Time: 34.63 s\n",
      "Epoch: 57 ; Loss 0.36447; Time: 35.17 s\n",
      "Epoch: 58 ; Loss 0.36438; Time: 35.67 s\n",
      "Epoch: 59 ; Loss 0.36479; Time: 36.16 s\n",
      "Epoch: 60 ; Loss 0.36467; Time: 36.82 s\n",
      "Epoch: 61 ; Loss 0.36487; Time: 37.61 s\n",
      "Epoch: 62 ; Loss 0.36496; Time: 38.16 s\n",
      "Epoch: 63 ; Loss 0.36481; Time: 38.70 s\n",
      "Epoch: 64 ; Loss 0.36453; Time: 39.23 s\n",
      "Epoch: 65 ; Loss 0.36459; Time: 39.78 s\n",
      "Epoch: 66 ; Loss 0.36509; Time: 40.36 s\n",
      "Epoch: 67 ; Loss 0.36476; Time: 41.04 s\n",
      "Epoch: 68 ; Loss 0.36479; Time: 41.98 s\n",
      "Epoch: 69 ; Loss 0.36467; Time: 42.72 s\n",
      "Epoch: 70 ; Loss 0.36492; Time: 43.38 s\n",
      "Epoch: 71 ; Loss 0.36482; Time: 44.01 s\n",
      "Epoch: 72 ; Loss 0.36496; Time: 44.53 s\n",
      "Epoch: 73 ; Loss 0.36490; Time: 45.10 s\n",
      "Epoch: 74 ; Loss 0.36507; Time: 45.85 s\n",
      "Epoch: 75 ; Loss 0.36510; Time: 46.57 s\n",
      "Epoch: 76 ; Loss 0.36475; Time: 47.24 s\n",
      "Epoch: 77 ; Loss 0.36477; Time: 47.79 s\n",
      "Epoch: 78 ; Loss 0.36487; Time: 48.52 s\n",
      "Epoch: 79 ; Loss 0.36463; Time: 49.11 s\n",
      "Epoch: 80 ; Loss 0.36454; Time: 49.68 s\n",
      "Epoch: 81 ; Loss 0.36469; Time: 50.26 s\n",
      "Epoch: 82 ; Loss 0.36481; Time: 50.80 s\n",
      "Epoch: 83 ; Loss 0.36531; Time: 51.36 s\n",
      "Epoch: 84 ; Loss 0.36476; Time: 52.02 s\n",
      "Epoch: 85 ; Loss 0.36502; Time: 52.68 s\n",
      "Epoch: 86 ; Loss 0.36467; Time: 53.37 s\n",
      "Epoch: 87 ; Loss 0.36504; Time: 54.14 s\n",
      "Epoch: 88 ; Loss 0.36478; Time: 54.66 s\n",
      "Epoch: 89 ; Loss 0.36468; Time: 55.32 s\n",
      "Epoch: 90 ; Loss 0.36479; Time: 55.85 s\n",
      "Epoch: 91 ; Loss 0.36474; Time: 56.39 s\n",
      "Epoch: 92 ; Loss 0.36477; Time: 56.94 s\n",
      "Epoch: 93 ; Loss 0.36490; Time: 57.51 s\n",
      "Epoch: 94 ; Loss 0.36505; Time: 58.05 s\n",
      "Epoch: 95 ; Loss 0.36451; Time: 58.64 s\n",
      "Epoch: 96 ; Loss 0.36480; Time: 59.23 s\n",
      "Epoch: 97 ; Loss 0.36502; Time: 59.85 s\n",
      "Epoch: 98 ; Loss 0.36508; Time: 60.73 s\n",
      "Epoch: 99 ; Loss 0.36498; Time: 61.27 s\n",
      "Epoch: 100 ; Loss 0.36501; Time: 62.13 s\n",
      "Epoch: 101 ; Loss 0.36494; Time: 62.75 s\n",
      "Epoch: 102 ; Loss 0.36509; Time: 63.47 s\n",
      "Epoch: 103 ; Loss 0.36482; Time: 64.09 s\n",
      "Epoch: 104 ; Loss 0.36515; Time: 64.78 s\n",
      "Epoch: 105 ; Loss 0.36469; Time: 65.64 s\n",
      "Epoch: 106 ; Loss 0.36487; Time: 66.43 s\n",
      "Epoch: 107 ; Loss 0.36494; Time: 67.03 s\n",
      "Epoch: 108 ; Loss 0.36469; Time: 67.58 s\n",
      "Epoch: 109 ; Loss 0.36481; Time: 68.10 s\n",
      "Epoch: 110 ; Loss 0.36461; Time: 68.68 s\n",
      "Epoch: 111 ; Loss 0.36439; Time: 69.27 s\n",
      "Epoch: 112 ; Loss 0.36490; Time: 70.01 s\n",
      "Epoch: 113 ; Loss 0.36490; Time: 70.57 s\n",
      "Epoch: 114 ; Loss 0.36476; Time: 71.12 s\n",
      "Epoch: 115 ; Loss 0.36493; Time: 71.80 s\n",
      "Epoch: 116 ; Loss 0.36484; Time: 72.58 s\n",
      "Epoch: 117 ; Loss 0.36470; Time: 73.47 s\n",
      "Epoch: 118 ; Loss 0.36507; Time: 74.18 s\n",
      "Epoch: 119 ; Loss 0.36453; Time: 74.98 s\n",
      "Epoch: 120 ; Loss 0.36490; Time: 75.70 s\n",
      "Epoch: 121 ; Loss 0.36500; Time: 76.31 s\n",
      "Epoch: 122 ; Loss 0.36465; Time: 76.91 s\n",
      "Epoch: 123 ; Loss 0.36492; Time: 77.54 s\n",
      "Epoch: 124 ; Loss 0.36506; Time: 78.18 s\n",
      "Epoch: 125 ; Loss 0.36485; Time: 78.78 s\n",
      "Epoch: 126 ; Loss 0.36486; Time: 79.34 s\n",
      "Epoch: 127 ; Loss 0.36489; Time: 79.98 s\n",
      "Epoch: 128 ; Loss 0.36506; Time: 80.49 s\n",
      "Epoch: 129 ; Loss 0.36464; Time: 81.13 s\n",
      "Epoch: 130 ; Loss 0.36489; Time: 81.98 s\n",
      "Epoch: 131 ; Loss 0.36453; Time: 82.66 s\n",
      "Epoch: 132 ; Loss 0.36504; Time: 83.52 s\n",
      "Epoch: 133 ; Loss 0.36493; Time: 84.26 s\n",
      "Epoch: 134 ; Loss 0.36484; Time: 84.97 s\n",
      "Epoch: 135 ; Loss 0.36522; Time: 85.80 s\n",
      "Epoch: 136 ; Loss 0.36508; Time: 86.54 s\n",
      "Epoch: 137 ; Loss 0.36492; Time: 87.38 s\n",
      "Epoch: 138 ; Loss 0.36518; Time: 88.02 s\n",
      "Epoch: 139 ; Loss 0.36501; Time: 88.56 s\n",
      "Epoch: 140 ; Loss 0.36510; Time: 89.08 s\n",
      "Epoch: 141 ; Loss 0.36537; Time: 89.66 s\n",
      "Epoch: 142 ; Loss 0.36521; Time: 90.25 s\n",
      "Epoch: 143 ; Loss 0.36486; Time: 90.81 s\n",
      "Epoch: 144 ; Loss 0.36521; Time: 91.35 s\n",
      "Epoch: 145 ; Loss 0.36446; Time: 91.96 s\n",
      "Epoch: 146 ; Loss 0.36480; Time: 92.52 s\n",
      "Epoch: 147 ; Loss 0.36485; Time: 93.19 s\n",
      "Epoch: 148 ; Loss 0.36484; Time: 94.00 s\n",
      "Epoch: 149 ; Loss 0.36470; Time: 94.57 s\n",
      "Epoch: 150 ; Loss 0.36493; Time: 95.14 s\n",
      "Epoch: 151 ; Loss 0.36501; Time: 95.78 s\n",
      "Epoch: 152 ; Loss 0.36496; Time: 96.48 s\n",
      "Epoch: 153 ; Loss 0.36514; Time: 97.19 s\n",
      "Epoch: 154 ; Loss 0.36506; Time: 97.78 s\n",
      "Epoch: 155 ; Loss 0.36507; Time: 98.49 s\n",
      "Epoch: 156 ; Loss 0.36454; Time: 99.23 s\n",
      "Epoch: 157 ; Loss 0.36494; Time: 99.95 s\n",
      "Epoch: 158 ; Loss 0.36508; Time: 100.75 s\n",
      "Epoch: 159 ; Loss 0.36473; Time: 101.42 s\n",
      "Epoch: 160 ; Loss 0.36499; Time: 102.26 s\n",
      "Epoch: 161 ; Loss 0.36502; Time: 102.79 s\n",
      "Epoch: 162 ; Loss 0.36502; Time: 103.63 s\n",
      "Epoch: 163 ; Loss 0.36514; Time: 104.41 s\n",
      "Epoch: 164 ; Loss 0.36498; Time: 104.89 s\n",
      "Epoch: 165 ; Loss 0.36500; Time: 105.54 s\n",
      "Epoch: 166 ; Loss 0.36447; Time: 106.15 s\n",
      "Epoch: 167 ; Loss 0.36495; Time: 106.89 s\n",
      "Epoch: 168 ; Loss 0.36524; Time: 107.74 s\n",
      "Epoch: 169 ; Loss 0.36470; Time: 108.37 s\n",
      "Epoch: 170 ; Loss 0.36513; Time: 109.22 s\n",
      "Epoch: 171 ; Loss 0.36484; Time: 109.98 s\n",
      "Epoch: 172 ; Loss 0.36490; Time: 110.63 s\n",
      "Epoch: 173 ; Loss 0.36488; Time: 111.34 s\n"
     ]
    }
   ],
   "source": [
    "train(net=net, loss=loss, dataloader=dataloader,\n",
    "      num_epoch=num_epoch, learning_rate=learning_rate,\n",
    "      weight_decay=weight_dacay, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = torch.Tensor(adata.X.A).to(device)\n",
    "_, z, m = net(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "singleCell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7b03ce8c7a30be49e5d195a686dd6d87b82d3273e605bd24af2c40167f57d0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
